{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Writeup | Self-Driving Car project - Deep Learning**\n",
    "**x** min read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Abstract — This notebook is the writeup of the Traffic Sign Recognition project.** We apply Deep Learning and Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the SELF-DRIVING CAR nanodegree program. The project is broken down into three steps, which are:   \n",
    ">- Step 1: Data Set Summary and Exploration\n",
    "- Step 2: Design and Test a Model Architecture\n",
    "- Step 3: Test a Model on New Images\n",
    "\n",
    "The model yielded the accuracy of **9x.xx%** with a loss of **xx.xx**, above the human performance of 98.81%, using 32x32 pre-proceeded input images.\n",
    "\n",
    "Beyond the initial requirements, I also implemented the Tensorboard features: embedding visualizer, summary{images, loss, accuracy, weights, biais}, a comparaison of different architectures, and compared the model result with Google Images search.\n",
    "\n",
    "Here is the link to the [PROJECT SPECIFICATION](https://review.udacity.com/#!/rubrics/481/view) and here to my [PROJECT CODE](https://github.com/chatmoon/Traffic-Sign-Classifier-Project/blob/master/_2_WIP/_JNBK_/_TSC-step2.1_170309-1557_WIP.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Data Set Summary & Exploration\n",
    "\n",
    "The goals here are the following:\n",
    "* Load the [data set](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset) and Summarize\n",
    "* Explore and visualize the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Load and summarize the data set  \n",
    "\n",
    "*The code for the loading of the data set is contained in the code cell [2] of the IPython notebook.   \n",
    "And the one for the summary is contained in the code cell [4].*   \n",
    "\n",
    "I used the numpy library to calculate summary statistics of the traffic signs data set:   \n",
    "* The size of training set is 34799\n",
    "* The size of validing set is 4410\n",
    "* The size of test set is 12630\n",
    "* The shape of a traffic sign image is (32, 32, 3)\n",
    "* The number of unique classes/labels in the data set is 43"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. An exploratory visualization of the dataset\n",
    "##### 1.2.1. Explore the data set\n",
    "\n",
    "*The code for this step is contained in the code cells from [6] to [8] of the IPython notebook.*\n",
    "\n",
    "In this part, we have three representations of the data set:   \n",
    "- fig.1: a list showing the number of occurence per traffic sign name\n",
    "- fig.2: a bar chart showing the number of occurence per class id\n",
    "- fig.3: a bar chart showing the distribution of these traffic sign images into the data set\n",
    "\n",
    ">![fig.1](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_1_showList.PNG)\n",
    ">![fig.2](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_2_showChart.png)\n",
    ">![fig.3](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_3_showDist.png)\n",
    "\n",
    "**Observations:**    \n",
    "- The fig.1 and 2 show there is a large disparity between traffic sign occurences.   \n",
    "Additional data should be created in order to rebalance the under represented classes.\n",
    "- The fig.3 shows that each class has been piled on top of the other.   \n",
    "The data set should be shuffled before training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2. Visualize the data set\n",
    "*The code for this step is contained in the code cell [9] of the IPython notebook.*\n",
    "\n",
    "In this part, we have also three types of visualization of the data set:   \n",
    "- fig.4: 43 random images, one per class and with their black-boxes\n",
    "- fig.5: 5 x 10, 5 random traffic signs, 10 images each \n",
    "- fig.6: a sprite image showing all the traffic sign in a single image\n",
    "\n",
    "> *fig.4: 43 random images, one per class and with their black-box*\n",
    "![fig.4](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataVisu_4_show43TS.png)\n",
    "> *fig.5: 5 x 10, 5 random traffic signs, 10 images each*\n",
    "![fig.5](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataVisu_5_show5TS.png)\n",
    "> *fig.6: a sprite image showing all the traffic sign in a single image*\n",
    "![fig.6](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_sp_xx_5984x5984.png)   \n",
    "> *fig.7: a sampling of challenging images to classify*\n",
    "![fig.7](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataExplo_4_challenges.PNG)\n",
    "\n",
    "**Observations:**   \n",
    "The fig.4 to 7 show how the images are of different qualities. Many images are either shaky, too dark or not having the same scale. There are variabilities such as viewpoint variations, lighting conditions (saturations, low-contrast), motion-blur, occlusions, sun glare, physical damage, colors fading. The classification of the traffic sign can be challenging and complex in this context. We should pre-proceed the images to decrease the impact of the mixed qualities of images.\n",
    "\n",
    "**Note:** the sprite image will be useful for the embedding visualization in TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Design and Test a Model Architecture\n",
    "\n",
    "#### 2.1. Preprocess the data set and generate additional data\n",
    "\n",
    "##### 2.1.1. Preprocess the data set\n",
    "*The code for this step is contained in the code cell **[XX]** of the IPython notebook.* \n",
    "\n",
    "> **Definition:** Pre-processing refers to techniques such as converting to grayscale, normalization, etc.\n",
    "\n",
    "In this part, I describe how I preprocessed the image data, what techniques were chosen and why I chose these techniques. You will also find below a overview of the preprocessing workflow in figure *fig.8* with images showing the output of each preprocessing technique.\n",
    "\n",
    "In summary, the preprocessing workflow generate five different types of image: 0RGB, 1GRAY, 2SHP, 3HST, 4CLAHE.\n",
    "\n",
    "> *fig.8: the preprocessing workflow*\n",
    "![fig.8](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataPPro%20flow_170610-1221.png)\n",
    "\n",
    "\n",
    "First of all, **all preprocessed images are centered and normalized** because during the training of the network we multiply weights to the initial input and add biases to cause activations and then backpropagate with the gradients to train (update) the model. In this process, we do not want the gradients go out of control. Then all preprocessed images are centered around zero by subtracting the mean, and normalized by dividing by the standard deviation. This technique doesn't change the content of the image. It avoids the values of weights and biases to get too big or to small. It tackles the numerical stability issue that occurs when several small values are added to big values (introducing a lot of errors) during the optimization of the Loss function.\n",
    "\n",
    "The two starting points of this approch are:\n",
    "- the following comments: \"the ConvNet was trained with full supervision on the colorimages of the GTSRB dataset and reached 98.97% accuracyon the phase 1 test set. After the end of phase 1, additional experiments with grayscale images established a new record accuracy of 99.17%\", Traffic Sign Recognition with Multi-Scale Convolutional Networks, Pierre Sermanet and Yann LeCun\n",
    "- the observation of the training set samples shows there are variabilities such as colors fading, lighting conditions (saturations, low-contrast), sun glare, motion-blur. To overcome a part of these variabilities and make the classification easier, I converted RGB images to grascale, then grayscale images have been sharpened, the histograms of sharpened images have been equilized, and the histograms of equilized images have been equilized adaptively with limited contrast\n",
    "\n",
    "The following functions from OpenCV have been used:\n",
    "- 1GRAY: [cvtColor](http://docs.opencv.org/2.4/modules/imgproc/doc/miscellaneous_transformations.html#cvtcolor)   \n",
    "- 2SHP: [filter2D](http://docs.opencv.org/2.4/modules/imgproc/doc/filtering.html#filter2d)   \n",
    "- 3HST: [equalizeHist](http://docs.opencv.org/2.4/modules/imgproc/doc/histograms.html#equalizehist) to improve the contrast   \n",
    "- 4CLAHE: [createCLAHE](http://docs.opencv.org/3.1.0/d5/daf/tutorial_py_histogram_equalization.html)   \n",
    "\n",
    "**Note:**\n",
    "- In addition, I also tried to blur the images but I got a better accuracy removing this last technic   \n",
    "- other realistic perturbations would probably also increase robustness of the model such as other affine transformations, brightness or adding some artificial occlusions. They would be implemented in the future sprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Generate additional data\n",
    "*The code for this step is contained in the code cell **[XX]** of the IPython notebook.* \n",
    "\n",
    "In this part, I describe how and why I generated additional data, what techniques were chosen and why I chose these techniques. You will also find below a workflow of the generation of additional data in figure fig.10 with a visualization of the jittered image in fig.9.\n",
    "\n",
    "I decided to generate additional data for two reasons:\n",
    "- first, the fig.2 shows the data set is imbalanced, i.e. the classes are not represented equally. In this case, the accuracy measures might be excellent accuracy on paper but it is only reflecting the underlying class distribution. For example, if the accuracy is 90% of the instances in Class-3 (Speed limit 50km/h) is because the models look at the data and cleverly decide that the best thing to do is to always predict “Class-3” and achieve high accuracy\n",
    "- secondly, the amount of data might be not sufficient for the model to generalise well in production with new data   \n",
    "\n",
    "\n",
    "To add more data to the the data set, I combined the two following techniques:\n",
    "- images are randomly picked and perturbed in position ([-2,2] pixels)\n",
    "- then they are pertubed in rotation ([-15,+15] degrees)   \n",
    "\n",
    "\n",
    "**Notes:**\n",
    "- in addition, I also tried to use the bounding box to crop the images and then perturbing them in scale ([.9,1.1] ratio). I got an accuracy around 93%, far below the human performance of 98.81%. I removed this last part and I get a better result at the end      \n",
    "- the observation of the training set in fig.6 shows the data set is a stack of several series of 30 similar images **with usually increasing scale**. It is for that reason I did not implement the simple perturbation in scale\n",
    "- there are 21 traffic signs that have a horizontal or a vertical axis of symmetry. Consequently, they are invariant to horizontal or vertical flipping. This technic would be implemented to add more data to the data set in the future sprint\n",
    "\n",
    "Here is an example of an original image and an augmented image:\n",
    "> *fig.9: augmented data example*\n",
    "![fig.9](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataJit_test4.png)\n",
    "\n",
    "I did not know how much the data would have to be raised to improve the accuracy or to overcome the overfitting problem. Then I created several set of augmented data such as each class has a least the following quantity (qty) of occurence: 500, 1000, 1500, 2000, 2500 and 3000 for each preprocessed type of image (see fig.10).\n",
    "> *fig.10: workflow of the generation of additional data*\n",
    "![fig.10](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_dataJIT_flow_170617-2336.png)   \n",
    "> *tab.11: the size of augmented training set*\n",
    "\n",
    "| Minimum quantity per class | 0 | 500 | 1000 | 1500 | 2000 | 2500 | 3000 |\n",
    "|:---------------------------------------------------------------:| \n",
    "| Quantity in addion | 0 | 4440 | 12451 | 15690 | 18630 | 21490 | 21500 |\n",
    "| Total size | 34799 | 39239 | 51690 | 67380 | 86010 | 107500 | 129000 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Model Architecture   \n",
    "*The code for this step is contained in the code cell **[XX]** of the IPython notebook.*   \n",
    "\n",
    "In this part, I describe what the final model architecture used looks like, how the model has been trained and the approach taken for finding a solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.1. The final model architecture\n",
    "\n",
    "*The code for this step is contained in the code cell **[XX]** of the IPython notebook.*   \n",
    "\n",
    "Even though I have built and experimented [several models](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png), I get the best test and validation accuracies with the Lenet5 architecture.\n",
    "\n",
    "> *fig.12: the original LeNet5 architecture*\n",
    "![fig.12](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/lenet5.png)\n",
    "\n",
    "Here is a diagram describing the final model:\n",
    "> *fig.13: the final model architecture used*\n",
    "![fig.13](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428_mod1.png)\n",
    "\n",
    "The model consisted of the following layers:   \n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t | \n",
    "|:---------------------:|:----------------------------------------------:| \n",
    "| Input         \t\t| 32x32xch, ch=3 for RGB image or 1 for grayscale| \n",
    "| Convolution        \t| 1x1 stride, valid padding, outputs 28x28x6   \t |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 14x14x6  \t\t\t\t |\n",
    "| Convolution   \t    | 1x1 stride, valid padding, outputs 10x10x16\t |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 5x5x16  \t\t\t\t     |\n",
    "| Flatten\t      \t\t| outputs 400  \t\t\t\t    \t\t\t     |\n",
    "| Fully connected\t\t| outputs 200  \t\t\t\t    \t\t\t     |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t |\n",
    "| Fully connected\t\t| outputs 84  \t\t\t\t    \t\t\t     |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t |\n",
    "| Fully connected\t\t| outputs 43  \t\t\t\t    \t\t\t     |\n",
    "\n",
    "\n",
    "**Note:** The graph of the model can be found using Tensorboard: [graph](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_tensorboard.png) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2. How the model has been trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, the discussion includes the type of optimizer, the batch size, number of epochs and any hyperparameters such as learning rate.\n",
    "\n",
    "To train the model, I used the Adam optimyzer. Adam stands for Adaptive moment estimation. It usually outperforms the other types of optimizer like the gradient descent, the stochastic gradient descent, the mini-batch gradient descent, the momentum or the Nesterov accelerated gradient. For more details, see this great short [video](https://www.youtube.com/watch?v=nhqo0u1a6fw).   \n",
    "\n",
    "I used ReLU as activation function for the hidden layers. I implemented and tested leaky ReLU but the ReLU function gave me the best validation and test accuracies results. It outperforms the other activation functions like Sigmoid and hyperbolic tangent function (tanh). I did not try Maxout function. For more details, see this short [video](https://www.youtube.com/watch?v=-7scQpJT7uo).\n",
    "\n",
    "I used the dropout technique as regularization method with a rate of 0.67.\n",
    "\n",
    "Regarding the other hyperparameters:   \n",
    "\n",
    "| Hyperparameter    \t| Value  |   \n",
    "|:---------------------:|:------:|   \n",
    "| LEARNING RATE       \t| 8.5e-4 |   \n",
    "| EPOCHS   \t            | 100\t |   \n",
    "| BATCH SIZE            | 100\t |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3. The approach taken for finding a solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.0. Description of the general approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was an iterative approach.   \n",
    "\n",
    "As a prelude, I used the Lenet5 architecture with the initial RGB images as input data and arbitrary hyperparameter values.   \n",
    "I chose this starting point for two reasons:   \n",
    "- Firstly, the initial measure gave me a benchmark value of the validation accuracy that would serve as an element of comparaison when optimizing the architecture performance. At this stage, I got a validation accuracy of 91.7%, a test accuracy of 90.3% and a loss of 0.456 with a learning rate of 1E-3.  \n",
    "- Secondly, the Lenet5 architecture is a building block of the Multi-Scale Convolutional Network. ![ MultiScaleConvNet](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/9ab0de951cc9cdf16887b1f841f8da6affc9c0de/1-Figure2-1.png)   \n",
    "The latter promises a validation accuracy of 99.17%, above the human performance of 98.81%. In this way, I was able to get a sense of the impact of each parameter on a simpler system before playing with more complex architectures.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I played with the following parameters to tune the Lenet5 model:   \n",
    "- learning rate: [1E-3, 2E-3, 9E-4, 1E-4, 1E-5, 8E-4, 9.5E-4, 8.5E-4]\n",
    "- dropout rate: [0.5, 0.75, 0.25, 0.85, 0.6, 0.8, 0.67]\n",
    "- preprocessed data types: (centered & normalized) AND [RGB, grayscale, sharpen, histogram, CLAHE]\n",
    "- the amount of jittered data: [500, 1000, 1500, 2000, 2500, 3000]   \n",
    "- activation function: [Relu, leakyRelu]\n",
    "\n",
    "As a result, I got a better performance: a validation accuracy of 97.7%, a test accuracy of 95.8% with a loss of 0.141.\n",
    "\n",
    "Finally, I played with various changes in the initial architecture and tried other ones.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.1. Initial benchmark values with the Lenet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this stage, I got a validation accuracy of 91.7%, a test accuracy of 90.3% and a loss of 0.456 with a learning rate of 1E-3.   \n",
    "\n",
    "The fig.14.a shows the model does not overfit or underfit.     \n",
    "\n",
    "> *fig.14.a: model1 - cost and accuracies measures before, in between and tuning*\n",
    "![fig.14.a](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/4521055bf8aaa1f74f97d2eb83f0853ac51567f3/images/_TensorBoard_Mod1_variousLearningRate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.2. Tuning of the Lenet5 architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the learning rate, I was able to get a validation accuracy of 94.4% with a learning rate of 8.5E-4 *(see fig. 15.a)*.   \n",
    "\n",
    "A learning rate of 8.5E-4 would be kept for the next measures.\n",
    "\n",
    "> *fig.15.a: model1's cost and accuracies measures with various learning rates*\n",
    "![fig.15.a]( https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousLearningRate.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the dropout rate, I got better performance: a validation accuracy of 94.7% and a test accuracy of 93.5% with a dropout rate of 0.67 *(see fig. 15.b)*.   \n",
    "\n",
    "> *fig.15.b: model1's cost and accuracies measures with various dropout rates*\n",
    "![fig.15.b]( https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousDropoutRates.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the preprocessed data types in input, I got an improvment using centered and normalized data, and with grayscale and RGB images *(see fig. 16)*:\n",
    "- grayscale & centered & normalized: validation accuracy of 96.3%, a test accuracy of 94.5%, a cost of 0.279   \n",
    "- RGB & centered & normalized: validation accuracy of 94.9%, a test accuracy of 94.7%, a cost of 0.492   \n",
    "\n",
    "> *fig.16: model1's cost and accuracies measures with various with various preprocessed data types*\n",
    "![fig.16](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousPproDataType.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the amount of jittered data in input, I got an improvment using at least 3000 centered and normalized RGB images per class: a validation accuracy of 97.7%, a test accuracy of 95.8%, a cost of 0.141 *(see fig. 17)*.   \n",
    "\n",
    "> *fig.17: model1's cost and accuracies measures with various with various amount of jittered grayscale & RGB data*\n",
    "![fig.17](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_variousJitData.png)\n",
    "*Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.3. Other architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by varying architectures, the Lenet5 architecture has still gotten the best performance results *(see fig.18, .19)*.   \n",
    "\n",
    "> *fig.18: cost and accuracies measures with various architectures*\n",
    "![fig.18](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterModXNew_result.png)\n",
    "*Find [here](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png) the details of each model.*   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *fig.19: different architectures descriptiion that have been tested*\n",
    "![fig.19](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1429.PNG)\n",
    "*Find [here](https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_architecture_170711-1428.png) the details of each model.*  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2.2.3.4. In conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must say that I am a bit desappointed. Even though I implemented the Multi-Scale Convolutional Network and used various pre-processing data techniques, the performance was not as good as the record of 99.17% promised by the paper. I expected to improve the performance by playing with various architectures but I was not successful despite my several attempts and due to some hardware limitions. Only the standardization, normalization and jitteration techniques have a significant impact on the validation and test accuracies. \n",
    "\n",
    "In conclusion, after building and experimenting seven models, the best validation and test accuracies have been achieved with the Lenet5 architecture, using 32x32x3, jittered, centered and normalized RGB images. **The model yielded a validation accuracy of 97.7%, a test accuracy of 95.8% with a loss of 0.141.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   <-- VOUS ETES ICI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sue tips from https://machinelearningmastery.com/improve-deep-learning-performance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResourceExaustedError: **[Notes]\"images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32323 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 2002003 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\" + RAM memory consumption calcul\n",
    "Notes: Why don't we train on images larger then 32x32?\n",
    "Memory usage. Since the discriminator has fully-connected layers after the convolutions, the output of the last convolution must be flattened to connect to the first fully-connected layer. The size of this output is dependent on the input image size, and blows up really quickly (e.g. For an input size of 64x64, going from 128 feature maps to a fully connected layer with 512 nodes, you need a connection with 64x64x128x512 = 268,435,456 weights). Because of this, training on paimages larger than 32x32 causes an out-of-memory error (at least on my machine).\n",
    "Source: github.com/dyelax/Adversarial_Video_Generation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Describe the approach taken for finding a solution and getting the validation set accuracy to be at least 0.93. Include in the discussion the results on the training, validation and test sets and where in the code these were calculated. Your approach may have been an iterative process, in which case, outline the steps you took to get to the final solution and **why you chose those steps**. Perhaps your solution involved an already well known implementation or architecture. In this case, discuss why you think the architecture is suitable for the current problem.*\n",
    "\n",
    "My final model results were:\n",
    "* training set accuracy of ?\n",
    "* validation set accuracy of ? \n",
    "* test set accuracy of ?\n",
    "\n",
    "If an iterative approach was chosen:\n",
    "* What was the first architecture that was tried and why was it chosen?\n",
    "* What were some problems with the initial architecture?\n",
    "* How was the architecture adjusted and why was it adjusted? Typical adjustments could include choosing a different model architecture, adding or taking away layers (pooling, dropout, convolution, etc), using an activation function or changing the activation function. One common justification for adjusting an architecture would be due to overfitting or underfitting. A high accuracy on the training set but low accuracy on the validation set indicates over fitting; a low accuracy on both sets indicates under fitting.\n",
    "* Which parameters were tuned? How were they adjusted and why?\n",
    "* What are some of the important design choices and why were they chosen? For example, why might a convolution layer work well with this problem? How might a dropout layer help with creating a successful model?\n",
    "\n",
    "If a well known architecture was chosen:\n",
    "* What architecture was chosen?\n",
    "* Why did you believe it would be relevant to the traffic sign application?\n",
    "* How does the final model's accuracy on the training, validation and test set provide evidence that the model is working well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.x. xxxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *fig.1x: model1's cost and accuracies measures with various hyperparameters and activations*\n",
    "![fig.1x]( \n",
    "https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/master/images/_scatterMod1_result.png)\n",
    "Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *fig.1y: cost and accuracies measures comparison of defferent models*\n",
    "![fig.1y]( https://raw.githubusercontent.com/chatmoon/Traffic_Sign_Classifier/0d70f968fea8dc52d5d942e805d9560ebb89f488/images/_scatterModX_result.png)\n",
    "Find [here](https://github.com/chatmoon/Traffic_Sign_Classifier/blob/master/jpbk/_SimulationResults_170725-1237.ipynb) the details of the model1's cost and accuracies measure.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![LeNet5](http://eblearn.sourceforge.net/lib/exe/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![LeNet](http://eblearn.sourceforge.net/lib/exe/lenet5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![ MultiScaleConvNet](https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/9ab0de951cc9cdf16887b1f841f8da6affc9c0de/1-Figure2-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Notes:**\n",
    "- Neural network architecture (is the network over or underfitting?)\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "\n",
    "Why would we need the zero-padding thing?\n",
    "\n",
    "\n",
    "**Notes:**\n",
    "\"images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully-connected neuron in a first hidden layer of a regular Neural Network would have 32*32*3 = 3072 weights. This amount still seems manageable, but clearly this fully-connected structure does not scale to larger images. For example, an image of more respectable size, e.g. 200x200x3, would lead to neurons that have 200*200*3 = 120,000 weights. Moreover, we would almost certainly want to have several such neurons, so the parameters would add up quickly! Clearly, this full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting.\" + RAM memory consumption calcul\n",
    "\n",
    "**Notes:**\n",
    "Why don't we train on images larger then 32x32?  \n",
    "> Memory usage. Since the discriminator has fully-connected layers after the convolutions, the output of the last convolution must be flattened to connect to the first fully-connected layer. The size of this output is dependent on the input image size, and blows up really quickly (e.g. For an input size of 64x64, going from 128 feature maps to a fully connected layer with 512 nodes, you need a connection with 64x64x128x512 = 268,435,456 weights). Because of this, training on paimages larger than 32x32 causes an out-of-memory error (at least on my machine).  \n",
    "  \n",
    "> Source: github.com/dyelax/Adversarial_Video_Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANNEX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[writeup_template.md](https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/writeup_template.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DELETE\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t | Comment |\n",
    "|:---------------------:|:----------------------------------------------:|:----------------------------------------------:|  \n",
    "| Input         \t\t| 32x32xch (ch=3 for RGB image, 1 for grayscale) | [32x32x3] will hold the raw pixel values of the image, in this case an image of width 32, height 32, and with three color channels R,G,B |\n",
    "| Convolution        \t| 1x1 stride, valid padding, outputs 28x28x6   \t | convolutional layer will compute the output of neurons that are connected to local regions in the input, each computing a dot product between their weights and a small region they are connected to in the input volume. This results in volume such as [28x28x6] |\n",
    "| RELU\t\t\t\t\t| \t\t\t\t\t\t\t\t\t\t\t\t | relu layer will apply an elementwise activation function. This leaves the size of the volume unchanged ([28x28x6]) |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 14x14x6  \t\t\t\t | max pooling layer will perform a downsampling operation along the spatial dimensions (width, height), resulting in volume such as [14x14x6] |\n",
    "| Convolution   \t    | 1x1 stride, valid padding, outputs 10x10x16\t | |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t | |\n",
    "| Max pooling\t      \t| 2x2 stride,  outputs 5x5x16  \t\t\t\t     | |\n",
    "| Flatten\t      \t\t| outputs 400  \t\t\t\t    \t\t\t     | |\n",
    "| Fully connected\t\t| outputs 200  \t\t\t\t    \t\t\t     | |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t | |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t | |\n",
    "| Fully connected\t\t| outputs 84  \t\t\t\t    \t\t\t     | |\n",
    "| RELU\t\t\t\t\t|\t\t\t\t\t\t\t\t\t\t\t\t | |\n",
    "| Dropout\t\t\t\t| keep_prob = 0.67\t\t\t\t\t\t\t\t | |\n",
    "| Fully connected\t\t| outputs 43  \t\t\t\t    \t\t\t     | fully-connected layer will compute the class scores, resulting in volume of size [1x1x43], where each of the 43 numbers correspond to a class score, such as among the 43 categories of the Traffic Sign dataset. Each neuron in this layer will be connected to all the numbers in the previous volume |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
